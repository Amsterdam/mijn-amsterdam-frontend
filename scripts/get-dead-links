#! /bin/bash

# This script makes outgoing request to all our external page links and -
# checks if these are still in use.
# Then it prints out all dead links found.

set -e

WITH_STATUS_CODE=false
if [ "$1" = '--with-status' ] || [ "$1" = '-s' ]; then
  WITH_STATUS_CODE=true
fi

EXTERNAL_URLS=$(
  grep --no-filename -r -P -o --exclude-dir="__snapshots__" --exclude="*test*" \
    "(\\\?[\`\"'])((https?:|www\.).*?)[^\\\]\1" ./src/* 2> /dev/null \
    | sed -E "s/([\`\"'])(.+)\1/\2/" \
    | sort --unique \
    | sed '/.*w3\.org.*/d' \
    | sed '/http:\/\/localhost*/d' \
    | sed '/http.*\/\/sap:.*/d'
)

echo_dead_link() {
  # Slice the URL for incredibly long matches spanning more then 10 lines.
  local url=${1:0:300}

  local status_code=$(curl \
    --globoff --output /dev/null --silent \
    --write-out "%{http_code}\n" \
    "$url")

  # There are also 000 codes, which can be caused by wrong certs or just an invalid URL.
  # I choose to leave these out since I did not see any problems with these URLS when manually checking in the browser.
  if [ "$status_code" -gt 399 ] && [ "$status_code" -lt 600 ]; then
    if [ $WITH_STATUS_CODE = true ]; then
      echo "Status: $status_code - $url"
    else
      echo "$url"
    fi
  fi
}

for url in $EXTERNAL_URLS
do
  echo_dead_link "$url" &
done

wait
