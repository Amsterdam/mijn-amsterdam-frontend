#! /bin/bash

# This script makes outgoing request to all our external page links and -
# checks if these are still in use.
# Then it prints out all dead links found.

set -e

EXTERNAL_URLS=$(
  grep --no-filename -r -P -o --exclude-dir="__snapshots__" --exclude="*test*" \
    "(\\\?[\`\"'])((https?:|www\.).*?)[^\\\]\1" ./src/* 2> /dev/null \
    | sed -E "s/([\`\"'])(.+)\1/\2/" \
    | sort --unique \
    | sed '/.*w3\.org.*/d' \
    | sed '/http:\/\/localhost*/d' \
    | sed '/http.*\/\/sap:.*/d'
)

# 'https://data.amsterdam.nl/api/v1/dataset-name <- k re\'turns all features'

echo_dead_link() {
  local url=${1:0:300}

  local status_code=$(curl \
    --globoff --output /dev/null --silent \
    --write-out "%{http_code}\n" \
    "$url")

  # There are also 000 codes, which can be caused by wrong certs or just an invalid URL.
  # I choose to leave these out since I did not see any problems with these URLS when manually checking.
  if [ "$status_code" -gt 399 ] && [ "$status_code" -lt 600 ]; then
    echo "Status: $status_code - $url"
    # echo "$url"
  fi
}

for url in $EXTERNAL_URLS
do
  echo_dead_link "$url" &
done

wait
